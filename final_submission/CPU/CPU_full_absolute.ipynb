{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3bfULVd4XQU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import statistics\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from mlperf_loadgen import (\n",
        "    TestSettings, TestScenario, TestMode,\n",
        "    QuerySample, QuerySampleResponse,\n",
        "    StartTest, QuerySamplesComplete,\n",
        "    ConstructQSL, ConstructSUT, DestroyQSL, DestroySUT,\n",
        "    LogSettings, LoggingMode\n",
        ")\n",
        "\n",
        "# === Configuration (Easily adjustable for notebook use) ===\n",
        "# Set to True to run accuracy test\n",
        "RUN_ACCURACY = True\n",
        "# Set to True to run performance test\n",
        "RUN_PERFORMANCE = True\n",
        "# Batch size for inference, low for not making the kernel crash\n",
        "BATCH_SIZE = 1\n",
        "# Number of images to use (set to None to use all images)\n",
        "# For quick tests, use a small number like 100 or 1000\n",
        "NUM_IMAGES = 10000  # None = use all available images\n",
        "# Minimum test duration in milliseconds (for performance test)\n",
        "MIN_DURATION_MS = 10000  # 10 seconds for quick tests\n",
        "# Minimum query count\n",
        "MIN_QUERY_COUNT = 50  # Small count for quick tests\n",
        "\n",
        "# === Paths ===\n",
        "#Relative\n",
        "#base_dir = Path(os.getcwd())\n",
        "#dataset_dir = base_dir / \"dataset/imagenet\"\n",
        "#image_dir = dataset_dir / \"val_flat\"\n",
        "#map_file = dataset_dir / \"val_map.txt\"\n",
        "#results_dir = base_dir / \"results/resnet50/Offline\"\n",
        "#onnx_model_path = \"resnet50_fp32.onnx\"\n",
        "\n",
        "#Absolutes\n",
        "base_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference\")\n",
        "dataset_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset\")\n",
        "image_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset/imagenet/val_flat/\")\n",
        "map_file = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset/imagenet/val_map.txt\")\n",
        "results_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/results/resnet50/Offline\")\n",
        "onnx_model_path = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/resnet50_fp32.onnx\")\n",
        "\n",
        "\n",
        "# Ensure results directory exists\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# This ensures MLPerf logging files are created in the correct location\n",
        "original_working_dir = os.getcwd()\n",
        "os.chdir(results_dir)\n",
        "#print(f\"Changed working directory to: {os.getcwd()}\")\n",
        "\n",
        "# === Load val_map.txt and select subset if configured ===\n",
        "with open(map_file) as f:\n",
        "    entries = [line.strip().split() for line in f]\n",
        "\n",
        "# Keep track of the original dataset size\n",
        "full_dataset_size = len(entries)\n",
        "\n",
        "# Apply NUM_IMAGES limitation if set\n",
        "if NUM_IMAGES is not None and NUM_IMAGES < len(entries):\n",
        "    print(f\" Using subset of {NUM_IMAGES} images (from {len(entries)} total)\")\n",
        "    # Use consistent random sampling for reproducibility\n",
        "    random.seed(42)\n",
        "    entries = random.sample(entries, NUM_IMAGES)\n",
        "\n",
        "image_paths = [image_dir / e[0] for e in entries]\n",
        "ground_truth = [int(e[1]) for e in entries]\n",
        "sample_indices = list(range(len(image_paths)))\n",
        "\n",
        "print(f\" Dataset size: {len(image_paths)} images\")\n",
        "\n",
        "# === Preprocessing ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# === Dataset ===\n",
        "class ImagenetDataset:\n",
        "    def __init__(self, image_paths, labels):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def load_samples(self, indices):\n",
        "        #print(f\"üì• Loading {len(indices)} samples into RAM...\")\n",
        "        #for idx in indices:\n",
        "        #    img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        #    tensor = transform(img).unsqueeze(0).numpy()\n",
        "        #    self.cache[idx] = (tensor, self.labels[idx])\n",
        "        #print(\" Sample loading complete\")\n",
        "\n",
        "    def unload_samples(self, indices):\n",
        "        #for idx in indices:\n",
        "        #    self.cache.pop(idx, None)\n",
        "\n",
        "    def get_sample(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        tensor = transform(img).unsqueeze(0).numpy()\n",
        "        return tensor, self.labels[idx]\n",
        "        #return self.cache[idx]\n",
        "\n",
        "dataset = ImagenetDataset(image_paths, ground_truth)\n",
        "\n",
        "# === ONNX Runtime ===\n",
        "providers = [\"CPUExecutionProvider\"]\n",
        "print(\" Running with CPUExecutionProvider only\")\n",
        "\n",
        "print(f\" Loading ONNX model: {onnx_model_path}\")\n",
        "session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "print(f\" Model loaded with input name: {input_name}, output name: {output_name}\")\n",
        "\n",
        "# === Performance Metrics Collection ===\n",
        "predictions = {}\n",
        "issued_sample_indices = set()\n",
        "latencies = []\n",
        "batch_times = []\n",
        "total_samples_processed = 0\n",
        "start_time = None\n",
        "end_time = None\n",
        "\n",
        "def issue_queries(query_samples):\n",
        "    global total_samples_processed, start_time, end_time\n",
        "\n",
        "    if start_time is None:\n",
        "        start_time = time.time()\n",
        "\n",
        "    indices = [qs.index for qs in query_samples]\n",
        "    tensors = [dataset.get_sample(i)[0] for i in indices]\n",
        "\n",
        "    for i in range(0, len(tensors), BATCH_SIZE):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        # Ensure we don't go out of bounds\n",
        "        end_idx = min(i + BATCH_SIZE, len(tensors))\n",
        "        mini_batch = np.vstack(tensors[i:end_idx])\n",
        "        batch_indices = indices[i:end_idx]\n",
        "        batch_queries = query_samples[i:end_idx]\n",
        "\n",
        "        # Run inference\n",
        "        outputs = session.run([output_name], {input_name: mini_batch})[0]\n",
        "        batch_end = time.time()\n",
        "\n",
        "        # Record batch processing time\n",
        "        batch_latency = batch_end - batch_start\n",
        "        batch_times.append(batch_latency)\n",
        "\n",
        "        # Per-sample latency (divide batch time by batch size)\n",
        "        per_sample_latency = batch_latency / len(batch_indices)\n",
        "        latencies.extend([per_sample_latency] * len(batch_indices))\n",
        "\n",
        "        # Process results\n",
        "        top1_preds = np.argmax(outputs, axis=1)\n",
        "        top5_preds = np.argsort(outputs, axis=1)[:, -5:][:, ::-1]\n",
        "\n",
        "        # Send responses\n",
        "        for j, (sample_idx, top1, top5) in enumerate(zip(batch_indices, top1_preds, top5_preds)):\n",
        "            issued_sample_indices.add(sample_idx)\n",
        "            predictions[sample_idx] = {\n",
        "                \"top1\": int(top1),\n",
        "                \"top5\": [int(x) for x in top5]\n",
        "            }\n",
        "            response = QuerySampleResponse(batch_queries[j].id, 0, 0)\n",
        "            QuerySamplesComplete([response])\n",
        "            total_samples_processed += 1\n",
        "\n",
        "            # Debug printing for performance mode to track progress\n",
        "            if total_samples_processed % 500 == 0:\n",
        "                print(f\"Processed {total_samples_processed*5} samples...\", end=\"\\r\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "def flush_queries():\n",
        "    pass\n",
        "\n",
        "def load_samples_to_ram(sample_indices):\n",
        "    dataset.load_samples(sample_indices)\n",
        "\n",
        "def unload_samples_from_ram(sample_indices):\n",
        "    dataset.unload_samples(sample_indices)\n",
        "\n",
        "def run_accuracy_test():\n",
        "    \"\"\"Run MLPerf accuracy test\"\"\"\n",
        "    global predictions, issued_sample_indices\n",
        "    predictions = {}\n",
        "    issued_sample_indices = set()\n",
        "\n",
        "    print(\"\\n Running ACCURACY test...\")\n",
        "\n",
        "    # Configure logging\n",
        "    log_settings = LogSettings()\n",
        "    log_settings.log_output.outdir = \".\"  # Use current directory (which is now results_dir)\n",
        "    log_settings.log_output.copy_summary_to_stdout = True\n",
        "    log_settings.log_output.copy_detail_to_stdout = True\n",
        "\n",
        "    # Check available logging modes and use an appropriate one\n",
        "    available_modes = dir(LoggingMode)\n",
        "    if \"ASYNC_WRITE_BACK\" in available_modes:\n",
        "        log_settings.log_mode = LoggingMode.ASYNC_WRITE_BACK\n",
        "    elif \"AsyncWriteBack\" in available_modes:\n",
        "        log_settings.log_mode = LoggingMode.AsyncWriteBack\n",
        "    else:\n",
        "        # Fall back to the default mode or another appropriate mode\n",
        "        print(\"Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\")\n",
        "\n",
        "    # Configure test settings for Offline scenario\n",
        "    settings = TestSettings()\n",
        "    settings.scenario = TestScenario.Offline\n",
        "    settings.mode = TestMode.AccuracyOnly\n",
        "    settings.min_query_count = MIN_QUERY_COUNT\n",
        "    settings.min_duration_ms = MIN_DURATION_MS\n",
        "\n",
        "    # For Offline scenario, we want maximum throughput\n",
        "    # Set a very high QPS to ensure maximum throughput without throttling\n",
        "    settings.offline_expected_qps = len(sample_indices) / (MIN_DURATION_MS / 1000)\n",
        "\n",
        "    # Run the test\n",
        "    sut = ConstructSUT(issue_queries, flush_queries)\n",
        "    qsl = ConstructQSL(\n",
        "        len(sample_indices), min(1024, len(sample_indices)),\n",
        "        load_samples_to_ram,\n",
        "        unload_samples_from_ram\n",
        "    )\n",
        "\n",
        "    StartTest(sut, qsl, settings)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy_results = calculate_accuracy()\n",
        "\n",
        "    # Cleanup\n",
        "    DestroyQSL(qsl)\n",
        "    DestroySUT(sut)\n",
        "\n",
        "    return accuracy_results\n",
        "\n",
        "def run_performance_test():\n",
        "    \"\"\"Run MLPerf performance test\"\"\"\n",
        "    global latencies, batch_times, total_samples_processed, start_time, end_time\n",
        "    global predictions, issued_sample_indices\n",
        "\n",
        "    predictions = {}\n",
        "    issued_sample_indices = set()\n",
        "    latencies = []\n",
        "    batch_times = []\n",
        "    total_samples_processed = 0\n",
        "    start_time = None\n",
        "    end_time = None\n",
        "\n",
        "    print(\"\\n Running PERFORMANCE test...\")\n",
        "\n",
        "    # Configure logging\n",
        "    log_settings = LogSettings()\n",
        "    log_settings.log_output.outdir = \".\"  # Use current directory (which is now results_dir)\n",
        "    log_settings.log_output.copy_summary_to_stdout = True\n",
        "    log_settings.log_output.copy_detail_to_stdout = True\n",
        "\n",
        "    # Check available logging modes and use an appropriate one\n",
        "    available_modes = dir(LoggingMode)\n",
        "    if \"ASYNC_WRITE_BACK\" in available_modes:\n",
        "        log_settings.log_mode = LoggingMode.ASYNC_WRITE_BACK\n",
        "    elif \"AsyncWriteBack\" in available_modes:\n",
        "        log_settings.log_mode = LoggingMode.AsyncWriteBack\n",
        "    else:\n",
        "        # Fall back to the default mode or another appropriate mode\n",
        "        print(\"Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\")\n",
        "\n",
        "    # Configure test settings for Offline scenario with maximum throughput\n",
        "    settings = TestSettings()\n",
        "    settings.scenario = TestScenario.Offline\n",
        "    settings.mode = TestMode.PerformanceOnly\n",
        "    settings.min_query_count = MIN_QUERY_COUNT\n",
        "    settings.min_duration_ms = MIN_DURATION_MS\n",
        "\n",
        "    # Important: Limit the performance queries to the same number of samples\n",
        "    # This ensures we're testing the same dataset subset\n",
        "    performance_count = len(sample_indices)\n",
        "\n",
        "    # For Offline scenario, we want maximum throughput\n",
        "    # Set a high but reasonable QPS, adjusting to our dataset size.\n",
        "    settings.offline_expected_qps = performance_count / (MIN_DURATION_MS / 1000)\n",
        "\n",
        "    # Run the test\n",
        "    sut = ConstructSUT(issue_queries, flush_queries)\n",
        "    qsl = ConstructQSL(\n",
        "        performance_count, min(1024, performance_count),\n",
        "        load_samples_to_ram,\n",
        "        unload_samples_from_ram\n",
        "    )\n",
        "\n",
        "    # Pre-load timestamps to ensure we get accurate measurements\n",
        "    print(\"Starting performance test timer...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Start the test\n",
        "    StartTest(sut, qsl, settings)\n",
        "\n",
        "    # Ensure we captured the end time\n",
        "    if end_time is None:\n",
        "        end_time = time.time()\n",
        "\n",
        "    # Force collection of any remaining data\n",
        "    if not latencies and total_samples_processed > 0:\n",
        "        print(\"No latencies collected during test, estimating from total time\")\n",
        "        avg_latency = (end_time - start_time) / total_samples_processed\n",
        "        latencies = [avg_latency] * total_samples_processed\n",
        "\n",
        "    if not batch_times and total_samples_processed > 0:\n",
        "        print(\"No batch times collected, using dummy values\")\n",
        "        batch_times = [avg_latency * BATCH_SIZE]\n",
        "\n",
        "    # Save performance statistics\n",
        "    print(f\" Processed {total_samples_processed} samples in {end_time - start_time:.2f} seconds\")\n",
        "    perf_stats = save_performance_stats()\n",
        "\n",
        "    # Cleanup\n",
        "    DestroyQSL(qsl)\n",
        "    DestroySUT(sut)\n",
        "\n",
        "    return perf_stats\n",
        "\n",
        "def calculate_accuracy():\n",
        "    \"\"\"Calculate and save accuracy metrics\"\"\"\n",
        "    if not predictions:\n",
        "        print(\" No predictions collected for accuracy calculation\")\n",
        "        return {\"top1_accuracy\": 0, \"top5_accuracy\": 0, \"samples\": 0}\n",
        "\n",
        "    top1 = 0\n",
        "    top5 = 0\n",
        "    for i in predictions:\n",
        "        gt = ground_truth[i]\n",
        "        pred_data = predictions.get(i, {})\n",
        "        top1_pred = pred_data.get(\"top1\", -1)\n",
        "        top5_list = pred_data.get(\"top5\", [])\n",
        "        if top1_pred == gt:\n",
        "            top1 += 1\n",
        "        if gt in top5_list:\n",
        "            top5 += 1\n",
        "\n",
        "    top1_acc = top1 / len(predictions) * 100 if predictions else 0\n",
        "    top5_acc = top5 / len(predictions) * 100 if predictions else 0\n",
        "\n",
        "    # Save accuracy results\n",
        "    accuracy_txt = Path(\"accuracy.txt\")\n",
        "    with open(accuracy_txt, \"w\") as f:\n",
        "        f.write(f\"Top-1 Accuracy: {top1_acc:.2f}%\\n\")\n",
        "        f.write(f\"Top-5 Accuracy: {top5_acc:.2f}%\\n\")\n",
        "        f.write(f\"Total samples: {len(predictions)}\\n\")\n",
        "    print(f\"\\n Accuracy written to {accuracy_txt}\")\n",
        "\n",
        "    acc_json = Path(\"mlperf_log_accuracy.json\")\n",
        "    with open(acc_json, \"w\") as f:\n",
        "        json.dump({\"top1\": top1_acc, \"top5\": top5_acc}, f, indent=2)\n",
        "    #print(f\" mlperf_log_accuracy.json saved to {acc_json}\")\n",
        "\n",
        "    return {\n",
        "        \"top1_accuracy\": top1_acc,\n",
        "        \"top5_accuracy\": top5_acc,\n",
        "        \"samples\": len(predictions)\n",
        "    }\n",
        "\n",
        "def save_performance_stats():\n",
        "    \"\"\"Save detailed performance statistics\"\"\"\n",
        "    if not latencies:\n",
        "        print(\" No performance data collected\")\n",
        "        return {}\n",
        "\n",
        "    # Calculate statistics\n",
        "    test_duration = end_time - start_time if start_time and end_time else 0\n",
        "    throughput = total_samples_processed / test_duration if test_duration > 0 else 0\n",
        "\n",
        "    # Ensure we have data to work with\n",
        "    if len(latencies) == 0:\n",
        "        print(\" No latency data collected, using estimates\")\n",
        "        latencies.append(test_duration / total_samples_processed if total_samples_processed > 0 else 0)\n",
        "\n",
        "    if len(batch_times) == 0:\n",
        "        print(\" No batch time data collected, using estimates\")\n",
        "        batch_times.append(test_duration / (total_samples_processed / BATCH_SIZE) if total_samples_processed > 0 else 0)\n",
        "\n",
        "    stats = {\n",
        "        \"total_samples\": total_samples_processed,\n",
        "        \"test_duration_seconds\": test_duration,\n",
        "        \"throughput_samples_per_second\": throughput,\n",
        "        \"latency_stats\": {\n",
        "            \"mean\": statistics.mean(latencies) * 1000,  # ms\n",
        "            \"median\": statistics.median(latencies) * 1000,  # ms\n",
        "            \"min\": min(latencies) * 1000,  # ms\n",
        "            \"max\": max(latencies) * 1000,  # ms\n",
        "            \"p90\": np.percentile(latencies, 90) * 1000,  # ms\n",
        "            \"p95\": np.percentile(latencies, 95) * 1000,  # ms\n",
        "            \"p99\": np.percentile(latencies, 99) * 1000,  # ms\n",
        "        },\n",
        "        \"batch_stats\": {\n",
        "            \"mean\": statistics.mean(batch_times) * 1000,  # ms\n",
        "            \"median\": statistics.median(batch_times) * 1000,  # ms\n",
        "            \"min\": min(batch_times) * 1000,  # ms\n",
        "            \"max\": max(batch_times) * 1000,  # ms\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Explicitly print key performance metrics\n",
        "    print(f\"\\n PERFORMANCE SUMMARY:\")\n",
        "    print(f\"Total samples: {stats['total_samples']}\")\n",
        "    print(f\"Duration: {stats['test_duration_seconds']:.2f} seconds\")\n",
        "    print(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/second\")\n",
        "    print(f\"Average latency: {stats['latency_stats']['mean']:.2f} ms/sample\")\n",
        "    print(f\"P90 latency: {stats['latency_stats']['p90']:.2f} ms\")\n",
        "\n",
        "    # Save performance results\n",
        "    perf_txt = Path(\"performance.txt\")\n",
        "    with open(perf_txt, \"w\") as f:\n",
        "        f.write(f\"===== PERFORMANCE RESULTS =====\\n\\n\")\n",
        "        f.write(f\"Scenario: Offline\\n\")\n",
        "        f.write(f\"Total samples processed: {stats['total_samples']}\\n\")\n",
        "        f.write(f\"Test duration: {stats['test_duration_seconds']:.2f} seconds\\n\")\n",
        "        f.write(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/second\\n\\n\")\n",
        "\n",
        "        f.write(f\"===== LATENCY (ms) =====\\n\")\n",
        "        f.write(f\"Mean: {stats['latency_stats']['mean']:.2f}\\n\")\n",
        "        f.write(f\"Median: {stats['latency_stats']['median']:.2f}\\n\")\n",
        "        f.write(f\"Min: {stats['latency_stats']['min']:.2f}\\n\")\n",
        "        f.write(f\"Max: {stats['latency_stats']['max']:.2f}\\n\")\n",
        "        f.write(f\"90th percentile: {stats['latency_stats']['p90']:.2f}\\n\")\n",
        "        f.write(f\"95th percentile: {stats['latency_stats']['p95']:.2f}\\n\")\n",
        "        f.write(f\"99th percentile: {stats['latency_stats']['p99']:.2f}\\n\\n\")\n",
        "\n",
        "        f.write(f\"===== BATCH PROCESSING TIME (ms) =====\\n\")\n",
        "        f.write(f\"Mean: {stats['batch_stats']['mean']:.2f}\\n\")\n",
        "        f.write(f\"Median: {stats['batch_stats']['median']:.2f}\\n\")\n",
        "        f.write(f\"Min: {stats['batch_stats']['min']:.2f}\\n\")\n",
        "        f.write(f\"Max: {stats['batch_stats']['max']:.2f}\\n\")\n",
        "\n",
        "    print(f\"\\n Performance statistics written to {perf_txt}\")\n",
        "\n",
        "    # Save detailed performance JSON\n",
        "    perf_json = Path(\"performance_stats.json\")\n",
        "    with open(perf_json, \"w\") as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "    #print(f\" Performance stats saved to {perf_json}\")\n",
        "\n",
        "    # Create a simple summary file that mlperf_loadgen might be expecting\n",
        "    summary_file = Path(\"mlperf_log_summary.txt\")\n",
        "    if not summary_file.exists():\n",
        "        print(f\"‚ö†Ô∏è MLPerf log summary not found, creating a basic version\")\n",
        "        with open(summary_file, \"w\") as f:\n",
        "            f.write(f\"MLPerf Inference - ResNet50 - Offline Scenario\\n\")\n",
        "            f.write(f\"Samples: {stats['total_samples']}\\n\")\n",
        "            f.write(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/sec\\n\")\n",
        "            f.write(f\"Mean latency: {stats['latency_stats']['mean']:.2f} ms\\n\")\n",
        "            f.write(f\"90th percentile latency: {stats['latency_stats']['p90']:.2f} ms\\n\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "# === Run the tests ===\n",
        "results = {\n",
        "    \"test_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"dataset\": {\n",
        "        \"size\": len(image_paths),\n",
        "        \"full_dataset_size\": full_dataset_size,\n",
        "        \"batch_size\": BATCH_SIZE\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\n MLPerf Inference ResNet50 - Offline Scenario\")\n",
        "print(f\"Dataset size: {len(image_paths)} images, Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "# Run accuracy test if enabled\n",
        "if RUN_ACCURACY:\n",
        "    accuracy_results = run_accuracy_test()\n",
        "    results[\"accuracy\"] = accuracy_results\n",
        "\n",
        "# Run performance test if enabled\n",
        "if RUN_PERFORMANCE:\n",
        "    performance_stats = run_performance_test()\n",
        "    results[\"performance\"] = performance_stats\n",
        "\n",
        "print(\"\\n MLPerf Inference test completed successfully\")\n",
        "print(f\"Results saved to: {os.getcwd()}\")\n",
        "\n",
        "# Check if the required MLPerf log files were generated\n",
        "summary_file = Path(\"mlperf_log_summary.txt\")\n",
        "detail_file = Path(\"mlperf_log_detail.json\")\n",
        "\n",
        "if not summary_file.exists():\n",
        "    print(f\" MLPerf log summary file not found at: {summary_file}\")\n",
        "\n",
        "if not detail_file.exists():\n",
        "    print(f\" MLPerf log detail file not found at: {detail_file}\")\n",
        "\n",
        "# Display brief summary of results\n",
        "if RUN_ACCURACY and \"accuracy\" in results:\n",
        "    acc = results[\"accuracy\"]\n",
        "    print(f\"\\nTop-1 Accuracy: {acc['top1_accuracy']:.2f}% ({acc['samples']} samples)\")\n",
        "    print(f\"Top-5 Accuracy: {acc['top5_accuracy']:.2f}%\")\n",
        "\n",
        "if RUN_PERFORMANCE and \"performance\" in results:\n",
        "    perf = results[\"performance\"]\n",
        "    if perf and \"throughput_samples_per_second\" in perf:\n",
        "        print(f\"\\nThroughput: {perf['throughput_samples_per_second']:.2f} samples/second\")\n",
        "        print(f\"Average latency: {perf['latency_stats']['mean']:.2f} ms/sample\")\n",
        "        print(f\"P90 latency: {perf['latency_stats']['p90']:.2f} ms\")\n",
        "\n",
        "# change back to original dir\n",
        "os.chdir(original_working_dir)\n",
        "\n",
        "results"
      ]
    }
  ]
}