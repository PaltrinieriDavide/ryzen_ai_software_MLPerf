{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ccc67c",
   "metadata": {},
   "source": [
    "###Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a24a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from mlperf_loadgen import (\n",
    "    TestSettings, TestScenario, TestMode,\n",
    "    QuerySample, QuerySampleResponse,\n",
    "    StartTest, QuerySamplesComplete,\n",
    "    ConstructQSL, ConstructSUT, DestroyQSL, DestroySUT,\n",
    "    LogSettings, LoggingMode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e781303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "# Set to True to run accuracy test\n",
    "RUN_ACCURACY = True\n",
    "# Set to True to run performance test\n",
    "RUN_PERFORMANCE = True\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Number of images to use (set to None to use all images)\n",
    "# For quick tests, use a small number like 100 or 1000\n",
    "NUM_IMAGES = None  # None = use all available images\n",
    "\n",
    "# Minimum test duration in milliseconds (for performance test)\n",
    "MIN_DURATION_MS = 10000  # 10 seconds for quick tests\n",
    "\n",
    "# Minimum query count\n",
    "MIN_QUERY_COUNT = 50  # Small count for quick tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Source paths\n",
    "\n",
    "# === Paths ===\n",
    "#Relative\n",
    "base_dir = Path(os.getcwd())\n",
    "dataset_dir = base_dir / \"dataset/imagenet\"\n",
    "image_dir = dataset_dir / \"val_flat\"\n",
    "map_file = dataset_dir / \"val_map.txt\"\n",
    "results_dir = base_dir / \"results/resnet50/Offline\"\n",
    "onnx_model_path = \"resnet50_fp32.onnx\"\n",
    "\n",
    "#Absolutes\n",
    "#base_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference\")\n",
    "#dataset_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset\")\n",
    "#image_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset/imagenet/val_flat/\")\n",
    "#map_file = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/dataset/imagenet/val_map.txt\")\n",
    "#results_dir = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/results/resnet50/Offline\")\n",
    "#onnx_model_path = Path(\"C:/Users/iisc/npucloud_userdata/giulio-m-polimi/ryzenaisw/image_classification/mlperf_inference/resnet50_fp32.onnx\")\n",
    "\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# This ensures MLPerf logging files are created in the correct location\n",
    "original_working_dir = os.getcwd()\n",
    "os.chdir(results_dir)\n",
    "#print(f\"Changed working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e337f",
   "metadata": {},
   "source": [
    "###Daset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetDataset:\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def load_samples(self, indices):\n",
    "        #print(f\"📥 Loading {len(indices)} samples into RAM...\")\n",
    "        for idx in indices:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            tensor = transform(img).unsqueeze(0).numpy()\n",
    "            self.cache[idx] = (tensor, self.labels[idx])\n",
    "        #print(\" Sample loading complete\")\n",
    "\n",
    "    def unload_samples(self, indices):\n",
    "        for idx in indices:\n",
    "            self.cache.pop(idx, None)\n",
    "\n",
    "    def get_sample(self, idx):\n",
    "        return self.cache[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load val_map.txt ===\n",
    "with open(map_file) as f:\n",
    "    entries = [line.strip().split() for line in f]\n",
    "\n",
    "# Keep track of the original dataset size\n",
    "full_dataset_size = len(entries)\n",
    "\n",
    "# === Apply NUM_IMAGES limitation if set ===\n",
    "if NUM_IMAGES is not None and NUM_IMAGES < len(entries):\n",
    "    print(f\" Using subset of {NUM_IMAGES} images (from {len(entries)} total)\")\n",
    "    # Use consistent random sampling for reproducibility\n",
    "    random.seed(42)\n",
    "    entries = random.sample(entries, NUM_IMAGES)\n",
    "\n",
    "image_paths = [image_dir / e[0] for e in entries]\n",
    "ground_truth = [int(e[1]) for e in entries]\n",
    "sample_indices = list(range(len(image_paths)))\n",
    "\n",
    "print(f\" Dataset size: {len(image_paths)} images\")\n",
    "\n",
    "# === Preprocessing ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# === Dataset Creation ===\n",
    "dataset = ImagenetDataset(image_paths, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d5559",
   "metadata": {},
   "source": [
    "###ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_providers = ort.get_available_providers()\n",
    "print(f\" Available ONNX providers: {available_providers}\")\n",
    "\n",
    "# Try to use NPU acceleration\n",
    "npu_providers = [p for p in available_providers if 'NPU' in p or 'GPU' in p or 'Vitis' in p]\n",
    "if npu_providers:\n",
    "    providers = [npu_providers[0], \"CPUExecutionProvider\"]\n",
    "    print(f\" Using NPU acceleration with {npu_providers[0]}\")\n",
    "elif \"VitisAIExecutionProvider\" in available_providers:\n",
    "    providers = [\"VitisAIExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    print(\" Using VitisAI Execution Provider\")\n",
    "elif \"CUDAExecutionProvider\" in available_providers:\n",
    "    providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    print(\" Using CUDA Execution Provider\")\n",
    "else:\n",
    "    providers = [\"CPUExecutionProvider\"]\n",
    "    print(\"⚠️ No acceleration provider found, using CPU only\")\n",
    "\n",
    "print(f\" Loading ONNX model: {onnx_model_path}\")\n",
    "session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(f\" Model loaded with input name: {input_name}, output name: {output_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e74ec",
   "metadata": {},
   "source": [
    "###Performance Metrics Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "issued_sample_indices = set()\n",
    "latencies = []\n",
    "batch_times = []\n",
    "total_samples_processed = 0\n",
    "start_time = None\n",
    "end_time = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff3ddf",
   "metadata": {},
   "source": [
    "## `issue_queries(query_samples)`\n",
    "Performs inference on a batch of input samples and returns the model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_queries(query_samples):\n",
    "    global total_samples_processed, start_time, end_time\n",
    "    \n",
    "    if start_time is None:\n",
    "        start_time = time.time()\n",
    "    \n",
    "    indices = [qs.index for qs in query_samples]\n",
    "    tensors = [dataset.get_sample(i)[0] for i in indices]\n",
    "    \n",
    "    for i in range(0, len(tensors), BATCH_SIZE):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        end_idx = min(i + BATCH_SIZE, len(tensors))\n",
    "        mini_batch = np.vstack(tensors[i:end_idx])\n",
    "        batch_indices = indices[i:end_idx]\n",
    "        batch_queries = query_samples[i:end_idx]\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run([output_name], {input_name: mini_batch})[0]\n",
    "        batch_end = time.time()\n",
    "        \n",
    "        # Record batch processing time\n",
    "        batch_latency = batch_end - batch_start\n",
    "        batch_times.append(batch_latency)\n",
    "        \n",
    "        # Per-sample latency (divide batch time by batch size)\n",
    "        per_sample_latency = batch_latency / len(batch_indices)\n",
    "        latencies.extend([per_sample_latency] * len(batch_indices))\n",
    "        \n",
    "        # Process results\n",
    "        top1_preds = np.argmax(outputs, axis=1)\n",
    "        top5_preds = np.argsort(outputs, axis=1)[:, -5:][:, ::-1]\n",
    "\n",
    "        # Send responses\n",
    "        for j, (sample_idx, top1, top5) in enumerate(zip(batch_indices, top1_preds, top5_preds)):\n",
    "            issued_sample_indices.add(sample_idx)\n",
    "            predictions[sample_idx] = {\n",
    "                \"top1\": int(top1),\n",
    "                \"top5\": [int(x) for x in top5]\n",
    "            }\n",
    "            response = QuerySampleResponse(batch_queries[j].id, 0, 0)\n",
    "            QuerySamplesComplete([response])\n",
    "            total_samples_processed += 1\n",
    "            \n",
    "            # Debug printing for performance mode to track progress\n",
    "            if total_samples_processed % 500 == 0:\n",
    "                print(f\"Processed {total_samples_processed*5} samples...\", end=\"\\r\")\n",
    "    \n",
    "    end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f46935",
   "metadata": {},
   "source": [
    "## `flush_queries()`\n",
    "Ensures all pending inference requests are processed (currently does nothing).\n",
    "\n",
    "## `load_samples_to_ram(sample_indices)`\n",
    "Loads the selected samples into RAM to prepare them for inference.\n",
    "\n",
    "## `unload_samples_from_ram(sample_indices)`\n",
    "Removes the specified samples from RAM to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_queries():\n",
    "    pass\n",
    "\n",
    "def load_samples_to_ram(sample_indices):\n",
    "    dataset.load_samples(sample_indices)\n",
    "\n",
    "def unload_samples_from_ram(sample_indices):\n",
    "    dataset.unload_samples(sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd0daf",
   "metadata": {},
   "source": [
    "## `run_accuracy_test()`\n",
    "Executes the accuracy evaluation by running inference on test data and comparing results to ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550371bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_accuracy_test():\n",
    "    \"\"\"Run MLPerf accuracy test\"\"\"\n",
    "    global predictions, issued_sample_indices\n",
    "    predictions = {}\n",
    "    issued_sample_indices = set()\n",
    "    \n",
    "    print(\"\\n Running ACCURACY test...\")\n",
    "    \n",
    "    # Configure logging\n",
    "    log_settings = LogSettings()\n",
    "    log_settings.log_output.outdir = \".\"  # Use current directory (which is now results_dir)\n",
    "    log_settings.log_output.copy_summary_to_stdout = True\n",
    "    log_settings.log_output.copy_detail_to_stdout = True\n",
    "    \n",
    "    # Check available logging modes and use an appropriate one\n",
    "    available_modes = dir(LoggingMode)\n",
    "    if \"ASYNC_WRITE_BACK\" in available_modes:\n",
    "        log_settings.log_mode = LoggingMode.ASYNC_WRITE_BACK\n",
    "    elif \"AsyncWriteBack\" in available_modes:\n",
    "        log_settings.log_mode = LoggingMode.AsyncWriteBack\n",
    "    else:\n",
    "        # Fall back to the default mode or another appropriate mode\n",
    "        print(\"Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\")\n",
    "    \n",
    "    # Configure test settings for Offline scenario\n",
    "    settings = TestSettings()\n",
    "    settings.scenario = TestScenario.Offline\n",
    "    settings.mode = TestMode.AccuracyOnly\n",
    "    settings.min_query_count = MIN_QUERY_COUNT\n",
    "    settings.min_duration_ms = MIN_DURATION_MS\n",
    "    \n",
    "    # For Offline scenario, we want maximum throughput\n",
    "    # Set a very high QPS to ensure maximum throughput without throttling\n",
    "    settings.offline_expected_qps = len(sample_indices) / (MIN_DURATION_MS / 1000)\n",
    "    \n",
    "    # Run the test\n",
    "    sut = ConstructSUT(issue_queries, flush_queries)\n",
    "    qsl = ConstructQSL(\n",
    "        len(sample_indices), min(1024, len(sample_indices)),\n",
    "        load_samples_to_ram,\n",
    "        unload_samples_from_ram\n",
    "    )\n",
    "    \n",
    "    StartTest(sut, qsl, settings)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_results = calculate_accuracy()\n",
    "    \n",
    "    # Cleanup\n",
    "    DestroyQSL(qsl)\n",
    "    DestroySUT(sut)\n",
    "    \n",
    "    return accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d3a91",
   "metadata": {},
   "source": [
    "## `run_performance_test()`\n",
    "Measures the inference speed and throughput of the model under test conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_test():\n",
    "    \"\"\"Run MLPerf performance test\"\"\"\n",
    "    global latencies, batch_times, total_samples_processed, start_time, end_time\n",
    "    global predictions, issued_sample_indices\n",
    "    \n",
    "    predictions = {}\n",
    "    issued_sample_indices = set()\n",
    "    latencies = []\n",
    "    batch_times = []\n",
    "    total_samples_processed = 0\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    \n",
    "    print(\"\\n Running PERFORMANCE test...\")\n",
    "    \n",
    "    # Configure logging\n",
    "    log_settings = LogSettings()\n",
    "    log_settings.log_output.outdir = \".\"  # Use current directory (which is now results_dir)\n",
    "    log_settings.log_output.copy_summary_to_stdout = True\n",
    "    log_settings.log_output.copy_detail_to_stdout = True\n",
    "    \n",
    "    # Check available logging modes and use an appropriate one\n",
    "    available_modes = dir(LoggingMode)\n",
    "    if \"ASYNC_WRITE_BACK\" in available_modes:\n",
    "        log_settings.log_mode = LoggingMode.ASYNC_WRITE_BACK\n",
    "    elif \"AsyncWriteBack\" in available_modes:\n",
    "        log_settings.log_mode = LoggingMode.AsyncWriteBack\n",
    "    else:\n",
    "        # Fall back to the default mode or another appropriate mode\n",
    "        print(\"Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\")\n",
    "    \n",
    "    # Configure test settings for Offline scenario with maximum throughput\n",
    "    settings = TestSettings()\n",
    "    settings.scenario = TestScenario.Offline\n",
    "    settings.mode = TestMode.PerformanceOnly\n",
    "    settings.min_query_count = MIN_QUERY_COUNT\n",
    "    settings.min_duration_ms = MIN_DURATION_MS\n",
    "    \n",
    "    # Important: Limit the performance queries to the same number of samples\n",
    "    # This ensures we're testing the same dataset subset\n",
    "    performance_count = len(sample_indices)\n",
    "    \n",
    "    # For Offline scenario, we want maximum throughput\n",
    "    # Set a high but reasonable QPS, adjusting to our dataset size.\n",
    "    settings.offline_expected_qps = performance_count / (MIN_DURATION_MS / 1000)\n",
    "    \n",
    "    # Run the test\n",
    "    sut = ConstructSUT(issue_queries, flush_queries)\n",
    "    qsl = ConstructQSL(\n",
    "        performance_count, min(1024, performance_count),\n",
    "        load_samples_to_ram,\n",
    "        unload_samples_from_ram\n",
    "    )\n",
    "    \n",
    "    # Pre-load timestamps to ensure we get accurate measurements\n",
    "    print(\"Starting performance test timer...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start the test\n",
    "    StartTest(sut, qsl, settings)\n",
    "    \n",
    "    # Ensure we captured the end time\n",
    "    if end_time is None:\n",
    "        end_time = time.time()\n",
    "    \n",
    "    # Force collection of any remaining data\n",
    "    if not latencies and total_samples_processed > 0:\n",
    "        print(\"No latencies collected during test, estimating from total time\")\n",
    "        avg_latency = (end_time - start_time) / total_samples_processed\n",
    "        latencies = [avg_latency] * total_samples_processed\n",
    "    \n",
    "    if not batch_times and total_samples_processed > 0:\n",
    "        print(\"No batch times collected, using dummy values\")\n",
    "        batch_times = [avg_latency * BATCH_SIZE]\n",
    "    \n",
    "    # Save performance statistics\n",
    "    print(f\" Processed {total_samples_processed} samples in {end_time - start_time:.2f} seconds\")\n",
    "    perf_stats = save_performance_stats()\n",
    "    \n",
    "    # Cleanup\n",
    "    DestroyQSL(qsl)\n",
    "    DestroySUT(sut)\n",
    "    \n",
    "    return perf_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84966de",
   "metadata": {},
   "source": [
    "## `calculate_accuracy()`\n",
    "Computes accuracy metrics (e.g., top-1, top-5) based on the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy():\n",
    "    \"\"\"Calculate and save accuracy metrics\"\"\"\n",
    "    if not predictions:\n",
    "        print(\" No predictions collected for accuracy calculation\")\n",
    "        return {\"top1_accuracy\": 0, \"top5_accuracy\": 0, \"samples\": 0}\n",
    "        \n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "    for i in predictions:\n",
    "        gt = ground_truth[i]\n",
    "        pred_data = predictions.get(i, {})\n",
    "        top1_pred = pred_data.get(\"top1\", -1)\n",
    "        top5_list = pred_data.get(\"top5\", [])\n",
    "        if top1_pred == gt:\n",
    "            top1 += 1\n",
    "        if gt in top5_list:\n",
    "            top5 += 1\n",
    "\n",
    "    top1_acc = top1 / len(predictions) * 100 if predictions else 0\n",
    "    top5_acc = top5 / len(predictions) * 100 if predictions else 0\n",
    "\n",
    "    # Save accuracy results\n",
    "    accuracy_txt = Path(\"accuracy.txt\")\n",
    "    with open(accuracy_txt, \"w\") as f:\n",
    "        f.write(f\"Top-1 Accuracy: {top1_acc:.2f}%\\n\")\n",
    "        f.write(f\"Top-5 Accuracy: {top5_acc:.2f}%\\n\")\n",
    "        f.write(f\"Total samples: {len(predictions)}\\n\")\n",
    "    print(f\"\\n Accuracy written to {accuracy_txt}\")\n",
    "\n",
    "    acc_json = Path(\"mlperf_log_accuracy.json\")\n",
    "    with open(acc_json, \"w\") as f:\n",
    "        json.dump({\"top1\": top1_acc, \"top5\": top5_acc}, f, indent=2)\n",
    "    #print(f\" mlperf_log_accuracy.json saved to {acc_json}\")\n",
    "    \n",
    "    return {\n",
    "        \"top1_accuracy\": top1_acc,\n",
    "        \"top5_accuracy\": top5_acc,\n",
    "        \"samples\": len(predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c6bdd",
   "metadata": {},
   "source": [
    "\n",
    "## `save_performance_stats()`\n",
    "Gathers and stores performance statistics such as latency, throughput, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_performance_stats():\n",
    "    \"\"\"Save detailed performance statistics\"\"\"\n",
    "    if not latencies:\n",
    "        print(\" No performance data collected\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate statistics\n",
    "    test_duration = end_time - start_time if start_time and end_time else 0\n",
    "    throughput = total_samples_processed / test_duration if test_duration > 0 else 0\n",
    "    \n",
    "    # Ensure we have data to work with\n",
    "    if len(latencies) == 0:\n",
    "        print(\" No latency data collected, using estimates\")\n",
    "        latencies.append(test_duration / total_samples_processed if total_samples_processed > 0 else 0)\n",
    "    \n",
    "    if len(batch_times) == 0:\n",
    "        print(\" No batch time data collected, using estimates\")\n",
    "        batch_times.append(test_duration / (total_samples_processed / BATCH_SIZE) if total_samples_processed > 0 else 0)\n",
    "    \n",
    "    stats = {\n",
    "        \"total_samples\": total_samples_processed,\n",
    "        \"test_duration_seconds\": test_duration,\n",
    "        \"throughput_samples_per_second\": throughput,\n",
    "        \"latency_stats\": {\n",
    "            \"mean\": statistics.mean(latencies) * 1000,  # ms\n",
    "            \"median\": statistics.median(latencies) * 1000,  # ms\n",
    "            \"min\": min(latencies) * 1000,  # ms\n",
    "            \"max\": max(latencies) * 1000,  # ms\n",
    "            \"p90\": np.percentile(latencies, 90) * 1000,  # ms\n",
    "            \"p95\": np.percentile(latencies, 95) * 1000,  # ms\n",
    "            \"p99\": np.percentile(latencies, 99) * 1000,  # ms\n",
    "        },\n",
    "        \"batch_stats\": {\n",
    "            \"mean\": statistics.mean(batch_times) * 1000,  # ms\n",
    "            \"median\": statistics.median(batch_times) * 1000,  # ms\n",
    "            \"min\": min(batch_times) * 1000,  # ms\n",
    "            \"max\": max(batch_times) * 1000,  # ms\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Explicitly print key performance metrics\n",
    "    print(f\"\\n PERFORMANCE SUMMARY:\")\n",
    "    print(f\"Total samples: {stats['total_samples']}\")\n",
    "    print(f\"Duration: {stats['test_duration_seconds']:.2f} seconds\")\n",
    "    print(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/second\")\n",
    "    print(f\"Average latency: {stats['latency_stats']['mean']:.2f} ms/sample\")\n",
    "    print(f\"P90 latency: {stats['latency_stats']['p90']:.2f} ms\")\n",
    "    \n",
    "    # Save performance results\n",
    "    perf_txt = Path(\"performance.txt\")\n",
    "    with open(perf_txt, \"w\") as f:\n",
    "        f.write(f\"===== PERFORMANCE RESULTS =====\\n\\n\")\n",
    "        f.write(f\"Scenario: Offline\\n\")\n",
    "        f.write(f\"Total samples processed: {stats['total_samples']}\\n\")\n",
    "        f.write(f\"Test duration: {stats['test_duration_seconds']:.2f} seconds\\n\")\n",
    "        f.write(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/second\\n\\n\")\n",
    "        \n",
    "        f.write(f\"===== LATENCY (ms) =====\\n\")\n",
    "        f.write(f\"Mean: {stats['latency_stats']['mean']:.2f}\\n\")\n",
    "        f.write(f\"Median: {stats['latency_stats']['median']:.2f}\\n\")\n",
    "        f.write(f\"Min: {stats['latency_stats']['min']:.2f}\\n\")\n",
    "        f.write(f\"Max: {stats['latency_stats']['max']:.2f}\\n\")\n",
    "        f.write(f\"90th percentile: {stats['latency_stats']['p90']:.2f}\\n\")\n",
    "        f.write(f\"95th percentile: {stats['latency_stats']['p95']:.2f}\\n\")\n",
    "        f.write(f\"99th percentile: {stats['latency_stats']['p99']:.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"===== BATCH PROCESSING TIME (ms) =====\\n\")\n",
    "        f.write(f\"Mean: {stats['batch_stats']['mean']:.2f}\\n\")\n",
    "        f.write(f\"Median: {stats['batch_stats']['median']:.2f}\\n\")\n",
    "        f.write(f\"Min: {stats['batch_stats']['min']:.2f}\\n\")\n",
    "        f.write(f\"Max: {stats['batch_stats']['max']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"\\n Performance statistics written to {perf_txt}\")\n",
    "    \n",
    "    # Save detailed performance JSON\n",
    "    perf_json = Path(\"performance_stats.json\")\n",
    "    with open(perf_json, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    #print(f\" Performance stats saved to {perf_json}\")\n",
    "    \n",
    "    # Create a simple summary file that mlperf_loadgen might be expecting\n",
    "    summary_file = Path(\"mlperf_log_summary.txt\")\n",
    "    if not summary_file.exists():\n",
    "        print(f\"⚠️ MLPerf log summary not found, creating a basic version\")\n",
    "        with open(summary_file, \"w\") as f:\n",
    "            f.write(f\"MLPerf Inference - ResNet50 - Offline Scenario\\n\")\n",
    "            f.write(f\"Samples: {stats['total_samples']}\\n\")\n",
    "            f.write(f\"Throughput: {stats['throughput_samples_per_second']:.2f} samples/sec\\n\")\n",
    "            f.write(f\"Mean latency: {stats['latency_stats']['mean']:.2f} ms\\n\")\n",
    "            f.write(f\"90th percentile latency: {stats['latency_stats']['p90']:.2f} ms\\n\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8308202",
   "metadata": {},
   "source": [
    "###Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006dcac-b872-4b74-a4d0-bbee55660c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Changed working directory to: C:\\Users\\iisc\\npucloud_userdata\\giulio-m-polimi\\ryzenaisw\\image_classification\\mlperf_inference\\results\\resnet50\\Offline\n",
      "📊 Dataset size: 50000 images\n",
      "📋 Available ONNX providers: ['VitisAIExecutionProvider', 'DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "✅ Using NPU acceleration with VitisAIExecutionProvider\n",
      "🔄 Loading ONNX model: C:\\Users\\iisc\\npucloud_userdata\\giulio-m-polimi\\ryzenaisw\\image_classification\\mlperf_inference\\resnet50_fp32.onnx\n",
      "✅ Model loaded with input name: input, output name: output\n",
      "\n",
      "🚀 MLPerf Inference ResNet50 - Offline Scenario\n",
      "Dataset size: 50000 images, Batch size: 8\n",
      "\n",
      "🔍 Running ACCURACY test...\n",
      "Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📥 Loading 848 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "Processed 50000 samples...\n",
      "✅ Accuracy written to accuracy.txt\n",
      "✅ mlperf_log_accuracy.json saved to mlperf_log_accuracy.json\n",
      "\n",
      "⚡ Running PERFORMANCE test...\n",
      "Warning: ASYNC_WRITE_BACK logging mode not found, using default mode\n",
      "🕒 Starting performance test timer...\n",
      "📥 Loading 1024 samples into RAM...\n",
      "✅ Sample loading complete\n",
      "📊 Processed 55000 samples in 1053.56 seconds\n",
      "\n",
      "📊 PERFORMANCE SUMMARY:\n",
      "Total samples: 55000\n",
      "Duration: 1053.56 seconds\n",
      "Throughput: 52.20 samples/second\n",
      "Average latency: 18.95 ms/sample\n",
      "P90 latency: 19.86 ms\n",
      "\n",
      "✅ Performance statistics written to performance.txt\n",
      "✅ Performance stats saved to performance_stats.json\n",
      "\n",
      "✅ MLPerf Inference test completed successfully\n",
      "Results saved to: C:\\Users\\iisc\\npucloud_userdata\\giulio-m-polimi\\ryzenaisw\\image_classification\\mlperf_inference\\results\\resnet50\\Offline\n",
      "✅ MLPerf log summary file generated: mlperf_log_summary.txt\n",
      "✅ MLPerf log detail file generated: mlperf_log_detail.json\n",
      "\n",
      "Top-1 Accuracy: 76.15% (50000 samples)\n",
      "Top-5 Accuracy: 92.87%\n",
      "\n",
      "Throughput: 52.20 samples/second\n",
      "Average latency: 18.95 ms/sample\n",
      "P90 latency: 19.86 ms\n",
      "🔄 Changed back to original working directory: C:\\Users\\iisc\\npucloud_userdata\\giulio-m-polimi\\ryzenaisw\\image_classification\\mlperf_inference\\results\\resnet50\\Offline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_time': '2025-05-16 19:00:38',\n",
       " 'dataset': {'size': 50000, 'full_dataset_size': 50000, 'batch_size': 8},\n",
       " 'accuracy': {'top1_accuracy': 76.146,\n",
       "  'top5_accuracy': 92.872,\n",
       "  'samples': 50000},\n",
       " 'performance': {'total_samples': 55000,\n",
       "  'test_duration_seconds': 1053.5606064796448,\n",
       "  'throughput_samples_per_second': 52.203926059627804,\n",
       "  'latency_stats': {'mean': 18.94911579652266,\n",
       "   'median': 19.091039896011353,\n",
       "   'min': 8.903145790100098,\n",
       "   'max': 27.698993682861328,\n",
       "   'p90': 19.856035709381104,\n",
       "   'p95': 19.87442374229431,\n",
       "   'p99': 20.48429846763611},\n",
       "  'batch_stats': {'mean': 151.59292637218127,\n",
       "   'median': 152.72831916809082,\n",
       "   'min': 71.22516632080078,\n",
       "   'max': 221.59194946289062}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"test_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset\": {\n",
    "        \"size\": len(image_paths),\n",
    "        \"full_dataset_size\": full_dataset_size,\n",
    "        \"batch_size\": BATCH_SIZE\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n MLPerf Inference ResNet50 - Offline Scenario\")\n",
    "print(f\"Dataset size: {len(image_paths)} images, Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Run accuracy test if enabled\n",
    "if RUN_ACCURACY:\n",
    "    accuracy_results = run_accuracy_test()\n",
    "    results[\"accuracy\"] = accuracy_results\n",
    "    \n",
    "# Run performance test if enabled  \n",
    "if RUN_PERFORMANCE:\n",
    "    performance_stats = run_performance_test()\n",
    "    results[\"performance\"] = performance_stats\n",
    "\n",
    "print(\"\\n MLPerf Inference test completed successfully\")\n",
    "print(f\"Results saved to: {os.getcwd()}\")\n",
    "\n",
    "# Check if the required MLPerf log files were generated\n",
    "summary_file = Path(\"mlperf_log_summary.txt\")\n",
    "detail_file = Path(\"mlperf_log_detail.json\")\n",
    "\n",
    "if not summary_file.exists():\n",
    "    print(f\" MLPerf log summary file not found at: {summary_file}\")\n",
    "    \n",
    "if not detail_file.exists():\n",
    "    print(f\" MLPerf log detail file not found at: {detail_file}\")\n",
    "\n",
    "# Display brief summary of results\n",
    "if RUN_ACCURACY and \"accuracy\" in results:\n",
    "    acc = results[\"accuracy\"]\n",
    "    print(f\"\\nTop-1 Accuracy: {acc['top1_accuracy']:.2f}% ({acc['samples']} samples)\")\n",
    "    print(f\"Top-5 Accuracy: {acc['top5_accuracy']:.2f}%\")\n",
    "    \n",
    "if RUN_PERFORMANCE and \"performance\" in results:\n",
    "    perf = results[\"performance\"]\n",
    "    if perf and \"throughput_samples_per_second\" in perf:\n",
    "        print(f\"\\nThroughput: {perf['throughput_samples_per_second']:.2f} samples/second\")\n",
    "        print(f\"Average latency: {perf['latency_stats']['mean']:.2f} ms/sample\")\n",
    "        print(f\"P90 latency: {perf['latency_stats']['p90']:.2f} ms\")\n",
    "\n",
    "# change back to original dir\n",
    "os.chdir(original_working_dir)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9a83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a8a9b56-adc3-4387-90d8-28f2e877344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iisc\\npucloud_userdata\\giulio-m-polimi\\ryzenaisw\\image_classification\\mlperf_inference\\results\\resnet50\\Offline\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847fa35-7f13-44fc-bdcc-62e99a6ebaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
