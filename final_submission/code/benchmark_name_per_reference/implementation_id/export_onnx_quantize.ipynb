{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e5376-890c-419e-bbf8-341dfd1fe1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPORT MODEL TO ONNX\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "# === Load pretrained ResNet-50 ===\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# === Example input tensor ===\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# === Export to ONNX ===\n",
    "onnx_path = \"resnet50_fp32.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    opset_version=12\n",
    ")\n",
    "\n",
    "print(f\"✅ Exported model to: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64e85d-1855-4053-9ed7-c0ab94ceea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx onnxruntime onnxruntime-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ad4e4-f8b5-411b-8cc0-0691535b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATIC QUANTIZATION\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "import onnx\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# === Dataset for calibration ===\n",
    "class ImagenetCalibrationReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir, num_images=100):\n",
    "        self.image_paths = list(Path(image_dir).glob(\"*.JPEG\"))[:num_images]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.enum_data = None\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(self._preprocess())\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def _preprocess(self):\n",
    "        for img_path in self.image_paths:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            tensor = self.transform(img).unsqueeze(0).numpy()\n",
    "            yield {\"input\": tensor.astype(np.float32)}\n",
    "\n",
    "# === Run quantization ===\n",
    "fp32_model_path = \"resnet50_fp32.onnx\"\n",
    "int8_model_path = \"resnet50_int8.onnx\"\n",
    "calibration_reader = ImagenetCalibrationReader(\"image_classification/mlperf_inference/dataset/imagenet/val_flat\", num_images=100)\n",
    "\n",
    "quantize_static(\n",
    "    model_input=fp32_model_path,\n",
    "    model_output=int8_model_path,\n",
    "    calibration_data_reader=calibration_reader,\n",
    "    quant_format=QuantType.QOperator\n",
    ")\n",
    "\n",
    "print(f\"✅ Quantized model saved to: {int8_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761691be-b89f-439c-9650-0b50c31028c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE TO CHECK IF QUANTIZED MODEL WORKS WITH CPU\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "model_path = \"resnet50_int8.onnx\"\n",
    "image_path = Path(\"mlperf_inference/datasets/imagenet/val_flat/ILSVRC2012_val_00000001.JPEG\")\n",
    "\n",
    "# === Preprocess image ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(img).unsqueeze(0).numpy()\n",
    "\n",
    "# === Run inference ===\n",
    "session = ort.InferenceSession(model_path)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "output = session.run([output_name], {input_name: input_tensor})[0]\n",
    "\n",
    "# === Show top-1 prediction\n",
    "predicted_idx = np.argmax(output)\n",
    "print(f\"✅ ONNX Inference successful! Top-1 class index: {predicted_idx}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
